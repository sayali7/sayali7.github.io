
-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2020
  img: graph_glove-min.png
  title: "Embedding Words in Non-Vector Space with Unsupervised Graph Learning"
  authors: "Max Ryabinin, Sergei Popov, Liudmila Prokhorenkova, <u>Elena Voita</u>"
  doc-url:
  conf_name: EMNLP
  conf_year: 2020
  url: "https://arxiv.org/pdf/2010.02598"
  code: "https://github.com/yandex-research/graph-glove"
  abstract: "It has become a de-facto standard to represent words as elements of a vector space (word2vec, GloVe). While this approach is convenient, it is unnatural for language: words form a graph with a latent hierarchical structure, and this structure has to be revealed and encoded by word embeddings. We introduce GraphGlove: unsupervised graph word representations which are learned end-to-end. In our setting, each word is a node in a weighted graph and the distance between words is the shortest path distance between the corresponding nodes. We adopt a recent method learning a representation of data in the form of a differentiable weighted graph and use it to modify the GloVe training algorithm. We show that our graph-based representations substantially outperform vector-based methods on word similarity and analogy tasks. Our analysis reveals that the structure of the learned graphs is hierarchical and similar to that of WordNet, the geometry is highly non-trivial and contains subgraphs with different local topology."


-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2020
  img: mdl_probes-min.png
  title: "Information-Theoretic Probing with Minimum Description Length"
  authors: "<u>Elena Voita</u> and Ivan Titov"
  doc-url:
  conf_name: EMNLP
  conf_year: 2020
  url: "https://arxiv.org/abs/2003.12298"
  code: "https://github.com/lena-voita/description-length-probing"
  blog: "https://lena-voita.github.io/posts/mdl_probes.html"
  abstract: "To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates 'the amount of effort' needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes."

